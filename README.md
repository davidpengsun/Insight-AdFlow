# Insight-AdFlow
AdFlow is a realtime bidding system for advertisers to deliver the personalized ads to target users deployed on AWS and developed by Dr. David P. Sun during his training in Insight Data Science Program. In this document I will discuss the motivation, design, challenges and results about the project.
##Motivations
One of the multi-million dollar problem in marketing and advertisement industry is the to search for the target consumers, find out their preference and deliver the most related products to them. And often the consumers' preference has a non-uniform distribution over time and geolocations at least. Some deeper insight into this problem involves the market segmentation, which could be achieved by learning the network exists among the consumers based on their preferences. These are the three basic questions in business: when, where and to whom should the advertisement be delivered? All of this would require a quantitative description of both the consumers and the porducts that are to be advertised. And as I will show later, with the help of convenient AWS platform, big data tools and all kinds of text data source generated through the social network, we could provide a feasible solution.

##Models
The product is designed to answer the forementioned questions: When, where and who? There is a bidding system that to let the advertisers to compete for the opportunnity to deliver the advertisements. The inputs are text messages like tweets and text descriptions of advertisements, as well as the bids from advertisers. The output includes: a) a product index that show each product's popularity among the consumers, b) a geological distribution of the product indeices, c) the records the bids won by each advertisers.

The FIRST step is to extract features from the text messages, and THEN select most probable consumer-product pairs to send to the bidding system. FINALLY the bidding system matches for consumer with the highest scored bid from the competing products/advertisers, and save the result and perform the next round of bids. All these steps are processed quickly through the realtime processing pipeline. In the second step, for each incoming message it involves its correlation calculation with all the products provided by the advertisers. I set up the feature vectors from the first step to be 10 dimensional and I also set up 500 advertisers. So each bid could be able to be processed within 10 ms. Then, the incoming messages could be setup at a rate of 100/s.

###Techniques
The first challenge comes from the right way to extract a feature vector from a text message, typically 10 tokens/words in size. I chose the Word2Vec model to act as a ruler to measure the feature vector fot both the text messages and the product. With the word vector space, the text message is represented by one vector and the product is another, such that the distance between each tweet and eatch product/ad is now calculable. For simplicity, I skipped the training phase of the word2vec but synthesized a volcabulary of 18k a random vector for each word in the volcabulary. The synthesized text messages are a random sampling of size 1 to 10 with replacement from the volcabulary. Each product is also assigned a random vector initially. Notice that the quality, size and the dimension of the word vector would significantly affect the performance and the accuracy of the model. Larger word vectors would need to grow the memory of the pipeline or to seperate the part out of the pipeline and use the spark machine learning lib to train your own model then applied to the streaming processing.

The second challenge comes from the events detection job. To minimize the noise of the correlation between a user's tweet and a product, a time-windowed average has to be applied to the all the tweets for one user with in a certain amount of time. This job has better be seperated from the previous feature extraction process to smooth the traffice of the pipeline.
