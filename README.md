# Insight-AdFlow
AdFlow is a realtime bidding system for advertisers to deliver the personalized ads to target users deployed on AWS and developed by Dr. David P. Sun during his training in Insight Data Science Program. In this document I will discuss the motivation, design, challenges and results about the project.
##Motivations
One of the multi-million dollar problems in marketing and advertisement industry is to search for the target consumers, find out their preferences and deliver to them the most related products. And often the consumers' preference have a non-uniform distribution over time and geolocations at least. Some deeper insights into this problem involves the market segmentation, which could be achieved by learning the network exists among the consumers based on their preferences. These are the three basic questions in business: when, where and to whom should the advertisements be delivered? To answer any of these questions would require a quantitative description of both the consumers and the porducts that are to be advertised. And as I will show in this project, with the help of convenient AWS platform, big data tools and all kinds of text data source generated through the social network, we could provide a feasible solution.

![alt tag](https://github.com/davidpengsun/Insight-AdFlow/blob/master/pipeline.png)

##Models
The product is designed to answer the aforementioned questions: when, where and whom? The key information about the consumers are extracted from the social network text messages such as Tweets. There is also a bidding system that allows the advertisers to compete and bid for the opportunnities to deliver the advertisements. The inputs are text messages like tweets and text descriptions of advertisements, as well as the bids from advertisers. The output includes: a) a product index that show each product's popularity among the consumers, b) a geological distribution of the product indeices, c) the records the bids won by each advertisers.

The first step is to extract features from the text messages. The kafka producer will generate a text message flow at 100/s. And the first spark streaming processor will extract a feature vector from each message on the flow of 1-second microbatch. After the extraction job, the vectors are sent to the next spark streaming processor. The first question is to choose the right technique to extract a feature vector from a text message, typically 10 tokens(words) in size. I chose the Word2Vec model to act as a ruler to measure the feature vector for both the text messages and the product. In the word vector space, the text message is represented by one vector and the product by another, such that the distance between each (tweet, product/ad) is now calculable. For simplicity, I skipped the training phase of the word2vec but synthesized a volcabulary of 18k a random vector for each word in the volcabulary. The synthesized text messages are a random sampling of size 1 to 10 with replacement from the volcabulary. Each product is also assigned a random vector initially. Notice that the quality, size and the dimension of the word vector would significantly affect the performance and the accuracy of the model. Larger word vectors would need to grow the memory of the pipeline or to seperate the part out of the pipeline and use the spark machine learning lib to train your own model then applied to the streaming processing.

The next streaming processing will select most probable consumer-product pairs to send to the bidding system. All the product vectors are predefined and loaded and broadcasted to all the workers. Each incoming text-message-generated vector will be used to calculate the cosine similarity with all the product vectors. The ones that have high enough similaries larger than .9 are selected. To minimize the noise of the correlation between a user's tweet and a product, a time-windowed average has to be applied to all the tweets for one user within a certain amount of time. This job has better be seperated from the previous feature extraction process to smooth the traffice of the pipeline and let them each have a seperate streaming cycle. For the windowed average, the spark streaming provided a window function while the default way is to use the traditional microbatch method. The way the window function handles streaming job is that it needs the calculations to be invertible avoid the redundant calculations. On the other hand, it could detect events across adjacent microbatchs. In this model, the calculations are not invertible. So I choose the microbatch method for the event detection process.

Next the bidding system matches for consumer with the highest scored bid from the competing products/advertisers, and save the result and perform the next round of bids. The third challenge is about the proper way to match the biddings with detected ad-delivery events. There are several ways to approach the problem. The first is to join two streams together such as (k, v1)'s from stream 1 and (k, v2)'s from stream 2, by giving all pairs of (k, (v1, v2))'s. The drawback is that it is relatively hard to get all the bids from bidders in this case since there is a limited time window in the microbatch during streaming. Another way to do this is to create a table in the cassandra database, and also create a streaming job to constantly updating all the incoming bids from advertisers in the cassandra table while keeping the size of the table as small as possible. Then at each microbatch streaming job, the table is loaded to provide all the biddings from all the advertisers. There is a cost due to the communication between streaming jobs and cassandra, but the completeness of the bidderss biddings is guarenteed. And the latter is more important in my opinion for the system design. All the winning bids are recorded to a cassandra table and also passed to Kafka for the batch job.

Finally, for the purpose of accuracy, there is a batch job calculating the balance for each advertiser and summarzing the historical statistics on the hourly granularity for each product/advertiser. 

##Results
I set up the feature vectors from the first step to be 10 dimensional and I also set up 500 advertisers. So each bid could be able to be processed within 10 ms. Then, the incoming messages could be setup at a rate of 100/s. On the consumer side, I generated 90,000 consumers. And all together, they generate a stream of 100 text messages per second. Each text message is comprised of 1 to 10 words. All the words are randomly sampled from a volcabulary of 18,000 distinct words. As the pipeline is running, the big 90k-consumer-times-500-advertiser correlation table is computed over time and the column average per product is constantly recorded into the cassandra for dashboard rendering. For an average bid price of .2 USD, the system is expected to generate a 5,000 USD revenue per hour on average.






